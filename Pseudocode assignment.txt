      Within auditory perception, the most comprehensive approach to perceptual organization has been provided by Bregman’s (1990) work on auditory scene analysis, which has focused on the fundamental question of how the auditory system separates individual sources within the overlapping real-world environments. In this work Bregman identifies two ways in which we organize our auditory world – primitive and schema-based processing. Primitive processing is described as the division of auditory information using regularly occurring bottom-up cues. Schema-based processing follows more of a top-down approach in which the auditory system perceives and organizes incoming auditory information based on pre-existing auditory knowledge.       To date, investigations of auditory scene analysis have been conducted primarily using low-level forms of auditory information, such as pitch spread, temporal onsets, and timbre. In contrast, far fewer studies have examined the top-down influences on auditory perceptual organization. In this regard, musical contexts provide an exceptionally potent arena for understanding auditory perceptual organization as music provides a variety of variables that can be manipulated to better understand both bottom-up and top-down influences on auditory perception and segregation. Within music, two critical top-down components that may be important organizational frameworks involve the tonality of a musical passage and the contour of a melody. In this project we aim to understand the impact of tonality on the perceptual organization and understanding of musical stimuli while simultaneously controlling for interonset intervals and pitch spread. Specifically, this project examines whether tonal structure enables listeners to more effectively and efficiently organize a complex auditory sequence, employing multiple tasks to assess the impact of tonal structure on perceptual organization.       There will be three tonality manipulations; tonally the same (C major and C major), tonally related (C major and G major) and tonally unrelated (C major and F# major). Twelve 8-note melodies will be written for each key, and the interleaved melody will be written and interleaved in the original melody as a factor of pitch spread (either small (3-5 semitone spread) or large (11-13 semitone spread)). The intervals between each original and interleaved tone will be averaged across all the melodies (in midi notation). Temporal timing will also be accounted for, with three interonset interval (IOI) manipulations. In total, there will be 72 melodies for each tonality manipulation (2 pitch spreads X 3 IOI’s X tonal relatedness (for example, 6 melodies in C major and 6 in G major)). Every melody was run through a key finding algorithm in order to determine the tonality and to ensure that (for example) the melody would really be perceived as belonging to the key of F# major. The “Melodies (corr and intervals)” spreadsheet shows the unique 8-note melodies (and their tonal correlations) for each tonal relatedness condition (labelled as “Corr C+C”, “Corr C+G”, and “Corr C+F#”).       The 8-note melodies were then interleaved with their associated tonal related key (see “IM C+C”, “IM C+G”, and “IM C+F#”). The beginning interleaved melody was equalled between the two keys (for example, for half the conditions, melodies in G were interleaved in C melodies, and vice versa for the other half of the conditions). See the “Interleaved Melodies” spreadsheet for the complete melodies (midi numbers).            In this experiment, participants will hear a series of interleaved melodies varying along the dimensions of Tonal Relatedness, Pitch Spread, and Interonset Interval, and will be asked to judge the degree of stream segregation in these melodies. Participants will rate (on a 6-point scale) the degree of stream segregation perceived (ranging from “easy to hear 1 melody” to “easy to hear 2 melodies”). Stimuli will be randomly presented (in terms of tonal relatedness, pitch spread, and interonset interval). Participants will be brought to the lab and tested individually on the same computer with the same headphones.       As previously mentioned, intervals were averaged across all the melodies and note names were converted into midi numbers. The original melodies will be transposed into one of the twelve major keys (and associated tonal relatedness keys) based on the interval averages of the original melodies.       PSEUDOCODE PART 1: MAKING THE MELODIES (completed before every participant)PROGRAM IntLvdMels:READ Interleaved Melodies #CSVfileDECLARE Transposition into new key (Trans)IF tone = n	ADD MIDI number #predetermined (one of 10 or 11 remaining keys)REPEAT	TransUNTIL 	All melodies completeSAVE IN STIMULI FOLDER test_sub_1_melENDPSEUDOCODE PART 2: RUNNING THE STUDYREAD (from stimuli folder) test_sub_1_melsREPEAT      RANDOM.SHUFFLE (test_sub_1_mels[, random]) #randomizing the presentation of melodies      WRITE “On a scale from 1 (easy to hear 1 melody) to 6 (easy to hear 2 melodies), what did you hear for each melody?”      RECORD responseUNTIL	All melodies presentedDATASET  = test_sub_1_respFUNCTION clear screen	